{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAlyFusSloT-"
   },
   "source": [
    "### **Softmax Function and Cross Entropy Loss in Pytorch:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nwSUoIIAl6gK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0yA0pPVm5oi",
    "outputId": "83f6d579-b1b4-46a4-b494-3e8c5e9174d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax using numpy: [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "# softmax using numpy\n",
    "def softmax(x):\n",
    "  return np.exp(x) / np.sum(np.exp(x),axis = 0) # e^x / sum(e^x)\n",
    "\n",
    "x = np.array([2.0,1.0,0.1])\n",
    "outputs = softmax(x)\n",
    "print(\"softmax using numpy:\",outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6AEurIZnVRG",
    "outputId": "3fdb77de-ffbb-43e2-89c1-78791362ca44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax using torch tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "# softmax using torch\n",
    "\n",
    "x = torch.tensor([2.0,1.0,0.1])\n",
    "outputs = torch.softmax(x,dim = 0)\n",
    "print(\"softmax using torch\",outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KISnUJTZBGzy"
   },
   "source": [
    "**CrossEntropy loss** can be used in binary and multiclass classification.\n",
    "\n",
    "> lower the loss , higher the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1IPEK-Enwxc",
    "outputId": "7da68a01-fc26-4a82-9e4c-ef46a2d34f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss1: 0.3567\n",
      "loss2: 2.3026\n"
     ]
    }
   ],
   "source": [
    "# crossentropy loss using numpy\n",
    "\n",
    "def cross_entropy(true,pred):\n",
    "  loss = -np.sum(true * np.log(pred)) # -1/N(y*log(y'))\n",
    "  return loss\n",
    "\n",
    "# y must be one hot encoded.\n",
    "# if class 0: [1 0 0], if class 1: [0 1 0], if class 2: [0 0 1]\n",
    "\n",
    "y = np.array([1,0,0])\n",
    "\n",
    "y_predG = np.array([0.7,0.2,0.1]) # good pred\n",
    "y_predB = np.array([0.1,0.3,0.6]) # bad pred\n",
    "l1 = cross_entropy(y,y_predG)\n",
    "l2 = cross_entropy(y,y_predB)\n",
    "print(f\"loss1: {l1:.4f}\")\n",
    "print(f\"loss2: {l2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxHXG0X2OCQ-"
   },
   "source": [
    "**Careful!!**\n",
    "\n",
    "> *nn.CrossEntropyLoss* applies on **nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss).** \n",
    "so, no softmax in last layer.\n",
    "\n",
    "> Y has class labels, not One-Hot!\n",
    "Y_pred has raw scores (logits),no softmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhCKI0gADshC",
    "outputId": "fbf16fe5-cdd8-4415-cfde-ef921cb3e5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4170299470424652\n",
      "1.840616226196289\n",
      "tensor([0])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# crossentropy loss using torch\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([0])\n",
    "\n",
    "# n_sample x n_classes = 1 x 3\n",
    "\n",
    "Y_predG = torch.tensor([[2.0,1.0,0.1]])\n",
    "Y_predB = torch.tensor([[0.5,2.0,0.3]])\n",
    "\n",
    "l1 = loss(Y_predG,Y)\n",
    "l2 = loss(Y_predB,Y)\n",
    "\n",
    "print(l1.item())\n",
    "print(l2.item())\n",
    "\n",
    "# get predicted class\n",
    "_,pred1 = torch.max(Y_predG,1)\n",
    "_,pred2 = torch.max(Y_predB,1)\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1RH7S9tQlTb",
    "outputId": "dae7c8fa-7a53-4918-d6bd-440ace33152b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.414427250623703\n",
      "1.6018242835998535\n",
      "tensor([2, 0, 1])\n",
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# increasing samples\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "Y = torch.tensor([2,0,1])\n",
    "\n",
    "# n_sample x n_classes = 3 x 3\n",
    "\n",
    "Y_predG = torch.tensor([[0.5,1.0,2.1],[2.0,1.0,0.1],[0.5,3.0,2.1]])\n",
    "Y_predB = torch.tensor([[2.0,1.0,0.1],[0.1,1.0,2.1],[0.1,3.0,0.1]])\n",
    "\n",
    "l1 = loss(Y_predG,Y)\n",
    "l2 = loss(Y_predB,Y)\n",
    "\n",
    "print(l1.item())\n",
    "print(l2.item())\n",
    "\n",
    "# get predicted class\n",
    "_,pred1 = torch.max(Y_predG,1)\n",
    "_,pred2 = torch.max(Y_predB,1)\n",
    "\n",
    "print(pred1)\n",
    "print(pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDRA-YyPtfTn"
   },
   "source": [
    "### **A multiclass problem in pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ApTJbQNgtFnc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetMultiClass(nn.Module):\n",
    "  def __init__(self,input_size,hidden_size,num_classes):\n",
    "    super(NeuralNetMultiClass,self).__init__()\n",
    "    # here define layers\n",
    "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.linear2 = nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "  def forward(self,x):\n",
    "    out = self.linear1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear2(out) \n",
    "    # no softmax at the end\n",
    "    return out\n",
    "\n",
    "model = NeuralNetMultiClass(input_size = 28*28,hidden_size = 5,num_classes = 3)\n",
    "criterion = nn.CrossEntropyLoss() # applies softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWv97ikGwKbn"
   },
   "source": [
    "### **A binaryclass problem in pytorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FCMqXYqlwNDM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetBinaryClass(nn.Module):\n",
    "  def __init__(self,input_size,hidden_size):\n",
    "    super(NeuralNetBinaryClass,self).__init__()\n",
    "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.linear2 = nn.Linear(hidden_size,1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    out = self.linear1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear2(out) \n",
    "    # sigmoid at the end\n",
    "    y_pred = torch.sigmoid(out)\n",
    "    return y_pred\n",
    "\n",
    "model = NeuralNetBinaryClass(input_size = 28*28,hidden_size = 5)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPVtJPjqyFd4"
   },
   "source": [
    "### **Activation Function**\n",
    "\n",
    "If there is no activation function then our model is simple stacked linear model, which can  not learn the complex function.\n",
    "\n",
    "Each weighted input is aplied on activation function so that model can learn complex (non linear) pattern and give better result. \n",
    "\n",
    "> *with ativation function our model can withstand on non linear data also.*\n",
    "\n",
    "> *step, relu,leaky relu, tanh, softmax,sigmoid are some ativation fn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cT0cRS0o3nCW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# option 1 : create nn models\n",
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self,input_size,hidden_size):\n",
    "    super(NeuralNet,self).__init__()\n",
    "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
    "    self.relu = nn.ReLU() # activation fn\n",
    "    self.linear2 = nn.Linear(hidden_size,1)\n",
    "    self.sigmoid = nn.Sigmoid() # activation fn\n",
    "\n",
    "  def forward(self,x):\n",
    "    out = self.linear1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.linear2(out) \n",
    "    out = self.sigmoid(out)\n",
    "    return out\n",
    "\n",
    "# option 2 : use activation function directly in forward pass (not in init)\n",
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self,input_size,hidden_size):\n",
    "    super(NeuralNetBinaryClass,self).__init__()\n",
    "    self.linear1 = nn.Linear(input_size,hidden_size)\n",
    "    self.linear2 = nn.Linear(hidden_size,1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    out = torch.relu(self.linear1(x))\n",
    "    out = torch.sigmoid(self.linear2(out))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY6ZFUURASsI"
   },
   "source": [
    "**some activation function available in:**\n",
    "\n",
    "**1) nn modules (used inside __init__):**\n",
    "nn.ReLU(),\n",
    "nn.Tanh(),\n",
    "nn.LeakyReLU(),\n",
    "nn.Softmax(),\n",
    "nn.Sigmoid()\n",
    "\n",
    "**2) torch package (used inside forward function):**\n",
    "torch.relu(),\n",
    "torch.sigmoid(),\n",
    "torch.softmax(),\n",
    "torch.tanh()\n",
    "\n",
    ">*if some fn are not avilable in torch. package, then they are available \n",
    "in **torch.nn.functional as F** package.*\n",
    "\n",
    ">F.LeakyReLU(),\n",
    "F.relu()....etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1OrmM3dBrl9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SoftmaxCrossentropyActivation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
