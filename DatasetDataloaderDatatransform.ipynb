{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVAkbkmLzi4m"
   },
   "source": [
    "##### **EPOCH** = 1 forward and backward pass of all training samples\n",
    "##### **BATCH SIZE** = no. of training samples in 1 forward and backward pass \n",
    "##### **NO. OF ITERATIONS** = no. of passes, each pass using [batch size] no. of samples\n",
    " > *Ex. 100 samples , batch size = 20---> 100/20 = 5 iteration for 1 epoch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lWpJ70UE0H5y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hBg17DIR0gGU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iCfdkq65N1u"
   },
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "JGwct_n40jWX",
    "outputId": "678f2fdf-2b25-4237-9870-9c7f835222ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "class WineDataset(Dataset):\n",
    "\n",
    "  def __init__(self):\n",
    "    # data loading\n",
    "    xy = np.loadtxt('wine.csv',delimiter=',',dtype=np.float32,skiprows=1)\n",
    "    self.x = torch.from_numpy(xy[:,1:])\n",
    "    self.y = torch.from_numpy(xy[:,[0]])\n",
    "    self.n_samples = xy.shape[0]\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "   return self.x[index],self.y[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.n_samples\n",
    "\n",
    "dataset = WineDataset()\n",
    "# just print\n",
    "first_data = dataset[0]\n",
    "features,labels = first_data\n",
    "print(features,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOQv0bCr50Ua"
   },
   "source": [
    "### **DataLoader**\n",
    "\n",
    "To perform all operations (optimization) on batchwise loaded data (if dataset is too large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_CHD8pV75IcL",
    "outputId": "b4627ce1-5f89-4af2-dd90-69e2f5540313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3640e+01, 3.1000e+00, 2.5600e+00, 1.5200e+01, 1.1600e+02, 2.7000e+00,\n",
      "         3.0300e+00, 1.7000e-01, 1.6600e+00, 5.1000e+00, 9.6000e-01, 3.3600e+00,\n",
      "         8.4500e+02],\n",
      "        [1.3860e+01, 1.3500e+00, 2.2700e+00, 1.6000e+01, 9.8000e+01, 2.9800e+00,\n",
      "         3.1500e+00, 2.2000e-01, 1.8500e+00, 7.2200e+00, 1.0100e+00, 3.5500e+00,\n",
      "         1.0450e+03],\n",
      "        [1.3050e+01, 2.0500e+00, 3.2200e+00, 2.5000e+01, 1.2400e+02, 2.6300e+00,\n",
      "         2.6800e+00, 4.7000e-01, 1.9200e+00, 3.5800e+00, 1.1300e+00, 3.2000e+00,\n",
      "         8.3000e+02],\n",
      "        [1.3170e+01, 2.5900e+00, 2.3700e+00, 2.0000e+01, 1.2000e+02, 1.6500e+00,\n",
      "         6.8000e-01, 5.3000e-01, 1.4600e+00, 9.3000e+00, 6.0000e-01, 1.6200e+00,\n",
      "         8.4000e+02]]) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset=dataset,batch_size=4,shuffle = True,num_workers=2)\n",
    "dataiter = iter(dataloader)\n",
    "data = dataiter.next()\n",
    "features,labels = data\n",
    "print(features,labels)\n",
    "\n",
    "# batch size = 4 ; we get 4 features and corrusponding 4 targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BeZyDS5C6Ww2",
    "outputId": "895b5445-fde4-4b1c-98e8-40974be4dc72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples,n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "T-c5jD-T_Dcz",
    "outputId": "8720a7ae-daad-48d1-e7f2-618eceb74887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2,step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2,step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2,step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2,step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2,step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2,step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2,step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2,step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2,step 45/45, inputs torch.Size([2, 13])\n",
      "epoch 2/2,step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2,step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2,step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2,step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2,step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2,step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2,step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2,step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2,step 45/45, inputs torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "  for i, (inputs,labels) in enumerate(dataloader):\n",
    "    # fwd,bwd,update\n",
    "    if (i+1) % 5 ==0:\n",
    "      print(\"epoch {0}/{1},step {2}/{3}, inputs {4}\".format(epoch+1,epochs,\n",
    "                                                i+1,n_iterations,inputs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUD9Par3Dcvt"
   },
   "source": [
    "### **Dataset Transforms**\n",
    "\n",
    "> ***torchvision.datasets*** provides MNIST, CIFER, COCO, Fashion-mnist etc.\n",
    "\n",
    "**Transforms** can be applied to PIL images, tensors, ndarrays, or custom data\n",
    "during creation of the DataSet.\n",
    "\n",
    "complete list of built-in transforms: \n",
    "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\n",
    "On Images\n",
    "---------\n",
    "CenterCrop, Grayscale, Pad, RandomAffine,\n",
    "RandomCrop, RandomHorizontalFlip, RandomRotation\n",
    "Resize, Scale\n",
    "\n",
    "On Tensors\n",
    "----------\n",
    "LinearTransformation, Normalize, RandomErasing\n",
    "\n",
    "Conversion\n",
    "----------\n",
    "ToPILImage: from tensor or ndrarray\n",
    "\n",
    "ToTensor : from numpy.ndarray or PILImage\n",
    "\n",
    "Generic\n",
    "-------\n",
    "Use Lambda \n",
    "\n",
    "Custom\n",
    "------\n",
    "Write own class\n",
    "\n",
    "Compose multiple Transforms\n",
    "---------------------------\n",
    "composed = transforms.Compose([Rescale(256),\n",
    "                               RandomCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "85tYwStFHmTC",
    "outputId": "56010585-2dcd-4860-905e-85b5d128130a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nimport torch\\nimport torchvision\\n\\ndataset = torchvision.datasets.MNIST(\\n  root = './data',transform = torchvision.transforms.ToTensor()\\n)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "  root = './data',transform = torchvision.transforms.ToTensor()\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmAZMB7YAiKE",
    "outputId": "2024087a-25cc-4192-a26c-c9796569495e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "**********\n",
      "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "**********\n",
      "tensor([5.6920e+01, 6.8400e+00, 9.7200e+00, 6.2400e+01, 5.0800e+02, 1.1200e+01,\n",
      "        1.2240e+01, 1.1200e+00, 9.1600e+00, 2.2560e+01, 4.1600e+00, 1.5680e+01,\n",
      "        4.2600e+03])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# write our own transform and apply it our own dataset\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "\n",
    "  def __init__(self,transform = None):\n",
    "    # data loading\n",
    "    xy = np.loadtxt('wine.csv',delimiter=',',dtype=np.float32,skiprows=1)\n",
    "    self.x = xy[:,1:]\n",
    "    self.y = xy[:,[0]]\n",
    "    self.n_samples = xy.shape[0]\n",
    "\n",
    "    self.transform = transform\n",
    "\n",
    "  def __getitem__(self,index):\n",
    "   sample = self.x[index],self.y[index]\n",
    "\n",
    "   if self.transform:\n",
    "     sample = self.transform(sample)\n",
    "\n",
    "   return sample\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.n_samples\n",
    "\n",
    "class ToTensor:\n",
    "  def __call__(self,sample):\n",
    "    inputs,targets = sample\n",
    "    return torch.from_numpy(inputs),torch.from_numpy(targets)\n",
    "\n",
    "class MulTransform:\n",
    "  def __init__(self,factor):\n",
    "    self.factor = factor\n",
    "\n",
    "  def __call__(self,sample):\n",
    "    inputs,targets = sample\n",
    "    inputs *= self.factor\n",
    "    return inputs,targets\n",
    "\n",
    "dataset = WineDataset(transform = ToTensor())\n",
    "# just print\n",
    "first_data = dataset[0]\n",
    "features,labels = first_data\n",
    "print(type(features),type(labels))\n",
    "print('*'*10)\n",
    "\n",
    "\n",
    "dataset = WineDataset(transform = None)\n",
    "# just print\n",
    "first_data = dataset[0]\n",
    "features,labels = first_data\n",
    "print(features)\n",
    "print(type(features),type(labels))\n",
    "print('*'*10)\n",
    "\n",
    "\n",
    "# using Compose transform (composition of 2 transform)\n",
    "composed = torchvision.transforms.Compose([ToTensor(),MulTransform(4)])\n",
    "dataset = WineDataset(transform = composed)\n",
    "# just print\n",
    "first_data = dataset[0]\n",
    "features,labels = first_data\n",
    "print(features)\n",
    "print(type(features),type(labels))\n",
    "print('*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qmtVBgZhDd1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DatasetDataloaderDatatransform.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
